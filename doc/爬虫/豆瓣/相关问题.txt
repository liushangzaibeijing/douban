一、需要爬取得url的规律
    https://www.douban.com/xxx  
	https://xxx.douban.com/  
	豆瓣的模块

	https://www.douban.com/group/  分组
	https://www.douban.com/update/topic/ 忽略 
	https://www.douban.com/partner/   广告
	https://www.douban.com/location/   同城
	https://www.douban.com/group/explore 探索
	https://www.douban.com/about 豆瓣其他
	https://www.douban.com/note/xxx  话题
	
	感兴趣的模块：
	https://music.douban.com
	https://movie.douban.com
	https://book.douban.com
2、爬取书籍中分页情况的处理
    1、爬取当前页的时候获取下一页的url信息
        获取页面中的最大分页数有可能为有当前书籍信息事没有的 例如文学中首页中最大页码为98可是98页没有书籍信息

    2、爬取地书籍 作者 译者出版社之间的信息 可以会有缺失


3、使用多线程频繁爬取豆瓣数据的时候 如果没有相关的爬取策略 ip会被封
        当你不使用Cookie进行抓取时，连续的多请求几次就会被封，而且是封IP地址，即无论你怎么换UA都会返回403错误。
        当你使用Cookie时，第一次请求豆瓣页面会返回一个名为bid的Cookie，以后的请求都会带此Cookie，但当你请求过于频繁时，此Cookie也会被封。但是此时不会封IP地址。
        当你不是很频繁的请求豆瓣时，即使不带Cookie，豆瓣每次都会返回一个bid。
        基于以上3点，可以大体得出我们的爬虫策略：

        （1）不带Cookie先请求一次，并将返回的Cookie进行保存，下边的所有请求都带此Cookie。

        （2）请求一段时间被封后，清除所有Cookie后，再次进行请求，返回可用Cookie，依次循环

        此策略应该有效，但没有经过验证。
        我在试验过程中，尝试random获取豆瓣需要的bid值，发现竟然也可用，即bid不需要通过请求获得，直接随机即可。
        此时策略就更简单了：随机bid值，并加入到Cookie中进行页面请求，当发现被封时，清除Cookie，随机更换bid值。

        douban  bid=mABx-QaEx5U bid=mABx-QaEx5U

4、获取的书籍信息中的 作者，译者， 出版社 出版日期，价格这些参数以数据/分隔
      1）、// 作者，译者， 出版社 出版日期，价格 中可能会缺失其中的一部分 所以需要进行判断
             需要找出其中的规律
      2） 如果碰到的信息 中包含/可能也会出现问题


5、添加了一个爬取出错的信息记录的保存，方便后期区定位问题
   1、在爬取出错后，捕获异常，并将相关的出错信息保存在数据库中
      问题：出现抛出失误后的数据无法保存到数据库中
      解决：考虑先使用error级别的错误日志去进行保存，通过查看日志来定位出错信息
